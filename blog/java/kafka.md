# Kafka

## kafka lÃ  gÃ¬? Táº¡i sao pháº£i dÃ¹ng kafka?

Kafka lÃ  má»™t ná»n táº£ng message publish/subscribe phÃ¢n tÃ¡n (distributed messaging system) mÃ£ nguá»“n má»Ÿ Ä‘Æ°á»£c xÃ¢y dá»±ng nháº±m má»¥c Ä‘Ã­ch xá»­ lÃ½ dá»¯ liá»‡u streaming theo thá»i gian thá»±c.

### Kafka thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng cho 2 má»¥c Ä‘Ã­ch chÃ­nh sau:

- Publish vÃ  subscribe cÃ¡c stream of record (luá»“ng dá»¯ liá»‡u)
- LÆ°u trá»¯ cÃ¡c stream of record theo thá»© tá»±
- Há»— trá»£ xá»­ lÃ½ stream of record theo thá»i gian thá»±c

## Æ¯u Ä‘iá»ƒm
- Open-source
- High-throughput: CÃ³ kháº£ nÄƒng xá»­ lÃ½ má»™t lÆ°á»£ng lá»›n thÃ´ng tin má»™t cÃ¡ch liÃªn tá»¥c, gáº§n nhÆ° khÃ´ng cÃ³ thá»i gian chá»
- High-frequency: CÃ³ thá»ƒ xá»­ lÃ½ cÃ¹ng lÃºc nhiá»u message vÃ  nhiá»u thá»ƒ loáº¡i topic
- Scalability: Dá»… dÃ ng má»Ÿ rá»™ng khi cÃ³ nhu cáº§u
- Tá»± Ä‘á»™ng lÆ°u trá»¯ message, dá»… dÃ ng kiá»ƒm tra láº¡i
- Community: CÃ´Ì£ng Ä‘Ã´Ì€ng Kafka hÃ´Ìƒ trÆ¡Ì£ phaÌt triÃªÌ‰n maÌ£nh meÌƒ viÌ€ laÌ€ opensource (maÌƒ nguÃ´Ì€n mÆ¡Ì‰) nÃªn hÃªÌ£ thÃ´Ìng luÃ´n Ä‘Æ°Æ¡Ì£c cÃ¢Ì£p nhÃ¢Ì£t vaÌ€ hoaÌ€n thiÃªÌ£n tÆ°Ì€ng ngaÌ€y.
## NhÆ°á»£c Ä‘iá»ƒm
- ChÆ°a cÃ³ bá»™ cÃ´ng cá»¥ giÃ¡m sÃ¡t hoÃ n chá»‰nh: CÃ³ nhiá»u tool khÃ¡c nhau nhÆ°ng má»—i tool chá»‰ Ä‘Ã¡p á»©ng má»™t tÃ­nh nÄƒng quáº£n lÃ½ nháº¥t Ä‘á»‹nh, cháº³ng háº¡n nhÆ°: Kafka tool (offset manager) GUI tool - quáº£n lÃ½ topic vÃ  consumer, Lense - há»— trá»£ query message, Akhq - toolbox quáº£n lÃ½ Kafka vÃ  view data bÃªn trong Kafka
- KhÃ´ng chá»n Ä‘Æ°á»£c topic theo wildcard: NgÆ°á»i dÃ¹ng sáº½ cáº§n pháº£i sá»­ dá»¥ng chÃ­nh xÃ¡c tÃªn topic Ä‘á»ƒ xá»­ lÃ½ message
- Giáº£m hiá»‡u suáº¥t: KÃ­ch thÆ°á»›c message tÄƒng khiáº¿n cho consumer vÃ  producer pháº£i compress vÃ  decompress message, tá»« Ä‘Ã³ lÃ m bá»™ nhá»› bá»‹ cháº­m Ä‘i, áº£nh hÆ°á»Ÿng Ä‘áº¿n throughput vÃ  hiá»‡u suáº¥t.
- Xá»­ lÃ½ cháº­m: ÄÃ´i khi sá»‘ lÆ°á»£ng queues trong Kafka cluster tÄƒng Ä‘á»™t biáº¿n khiáº¿n Kafka xá»­ lÃ½ cháº­m hÆ¡n.


=>  Má»™t sá»‘ lá»£i Ã­ch ná»•i báº­t cá»§a Kafka bao gá»“m:

- Kháº£ nÄƒng má»Ÿ rá»™ng cao: MÃ´ hÃ¬nh phÃ¢n vÃ¹ng nháº­t kÃ½ cá»§a Kafka cho phÃ©p dá»¯ liá»‡u cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n phá»‘i trÃªn nhiá»u mÃ¡y chá»§ vÃ  má»Ÿ rá»™ng mÃ¡y chá»§ khi cÃ³ nhu cáº§u.
  Tá»‘c Ä‘á»™ nhanh chÃ³ng: Viá»‡c xá»­ lÃ½ thÃ´ng qua tÃ¡ch cÃ¡c luá»“ng dá»¯ liá»‡u giÃºp cho tá»‘c Ä‘á»™ trá»Ÿ nÃªn nhanh hÆ¡n.
  Kháº£ nÄƒng chá»‹u lá»—i vÃ  Ä‘á»™ bá»n: Do cÃ¡c gÃ³i dá»¯ liá»‡u Ä‘Æ°á»£c sao chÃ©p vÃ  phÃ¢n phá»‘i trÃªn nhiá»u mÃ¡y chá»§ khÃ¡c nhau, nÃªn khi cÃ³ sá»± cá»‘ thÃ¬ dá»¯ liá»‡u sáº½ Ã­t gáº·p lá»—i vÃ  an toÃ n hÆ¡n.

## Má»™t sá»‘ khÃ¡i niá»‡m cÆ¡ báº£n trong Kafka

1. Producer

Producer lÃ  nhá»¯ng application produce data vÃ  gá»­i data tá»›i Kafka Server. Data nÃ y sáº½ lÃ  nhá»¯ng message cÃ³ Ä‘á»‹nh dáº¡ng, Ä‘Æ°á»£c gá»­i dÆ°á»›i dáº¡ng máº£ng byte tá»›i Kafka server. VÃ­ dá»¥ nhÆ° cÃ¡c báº¡n cÃ³ má»™t táº­p tin .txt chá»©a text bÃªn trong, chÃºng ta cÃ³ thá»ƒ dÃ¹ng Producer Ä‘á»ƒ Ä‘á»c tá»«ng dÃ²ng trong táº­p tin nÃ y rá»“i gá»­i tá»›i Kafka server.

2. Consumer

Kafka sá»­ dá»¥ng consumer Ä‘á»ƒ subscribe vÃ o topic, cÃ¡c consumer Ä‘Æ°á»£c Ä‘á»‹nh danh báº±ng cÃ¡c group name. Nhiá»u consumer cÃ³ thá»ƒ cÃ¹ng Ä‘á»c má»™t topic. Sau khi nháº­n Ä‘Æ°á»£c data, Consumer cÃ³ thá»ƒ thÃªm code Ä‘á»ƒ xá»­ lÃ½ data theo nhu cáº§u cá»§a mÃ¬nh.

3. Cluster

Kafka cluster lÃ  má»™t set cÃ¡c server, má»—i má»™t set nÃ y Ä‘Æ°á»£c gá»i lÃ  1 broker.

4. Broker

Broker lÃ  Kafka server, lÃ  cáº§u ná»‘i giá»¯a Message Publisher vÃ  Message Consumer, giÃºp chÃºng cÃ³ thá»ƒ trao Ä‘á»•i message vá»›i nhau.

5. Topic

Dá»¯ liá»‡u truyá»n trong Kafka theo topic, khi cáº§n truyá»n dá»¯ liá»‡u cho cÃ¡c á»©ng dá»¥ng khÃ¡c nhau thÃ¬ sáº½ táº¡o ra cÃ¡c topic khÃ¡c nhau.

6. Partitions

Kafka lÃ  má»™t distributed messaging system vÃ  chÃºng ta cÃ³ thá»ƒ setup Kafka server vá»›i cluster. Trong trÆ°á»ng há»£p má»™t topic nháº­n quÃ¡ nhiá»u message táº¡i cÃ¹ng má»™t thá»i Ä‘iá»ƒm, chÃºng ta cÃ³ thá»ƒ chia topic nÃ y thÃ nh nhá»¯ng partitions Ä‘Æ°á»£c share giá»¯a cÃ¡c Kafka server vá»›i nhau trong má»™t cluster Ä‘Æ°á»£c handle cÃ¡c message nÃ y.

Má»™t partition sáº½ small vÃ  independent vá»›i cÃ¡c partitions khÃ¡c. Sá»‘ lÆ°á»£ng partition cho má»—i topic thÃ¬ tuá»³ theo nhu cáº§u cá»§a á»©ng dá»¥ng mÃ  chÃºng ta cÃ³ thá»ƒ quyáº¿t Ä‘á»‹nh.

7. Consumer Group

Consumer group lÃ  má»™t group cÃ¡c Consumer consume message tá»« Kafka server. Má»—i má»™t Consumer Group sáº½ share vá»›i nhau viá»‡c handle message.

8. ZOOKEEPER: Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ quáº£n lÃ½ vÃ  bá»‘ trÃ­ cÃ¡c broker.


<img src="blog/java/img/kafka1.png" style="display: block; margin-right: auto; margin-left: auto;">

<img src="blog/java/img/kafka2.png" style="display: block; margin-right: auto; margin-left: auto;">


# Example about kafka

## Config: CÃ³ 2 cÃ¡ch config:

- config trong application.yml
- config trong file config vs @configuraion vÃ  @Bean (thá»±c cháº¥t náº¿u cÃ³ cáº£ 2 thÃ¬ file @Bean nÃ y sáº½ ghi Ä‘Ã¨ lÃªn fiel application.yml)

```java
=============================== way 1 ===============================
spring:
  kafka:
    consumer:
      bootstrap-servers: localhost:9092
      group-id: group_id
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer // ná»™i dung Ä‘c gá»­i lÃªn server dáº¡ng string vÃ  láº¯ng nghe vá» dáº¡ng string
    producer:
      bootstrap-servers: localhost:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer // ná»™i dung náº¿u sá»­ dá»¥ng á»Ÿ service nÃ y Ä‘á»ƒ gá»­i lÃªn server sáº½ á»Ÿ dáº¡ng json
      properties:
        topics:
          testTopic: test_topic_application_yml

```

```java
@Data
@Configuration
@ConfigurationProperties(prefix = "spring.kafka")
public class KafkaConfig {
    ProducerConfig producer;

    @Data
    public static class ProducerConfig {
        Properties properties;
    }

    @Data
    public static class Properties {
        Topics topics;
    }

    @Data
    public static class Topics {
        String testTopic;
    }
}

// get topic file name from application.yaml
kafkaConfig.getProducer().getProperties().getTopics().getTestTopic();
```


```java
=============================== way 2 ===============================
@Configuration
class KafkaConsumerConfig {

    @Value("localhost:9092")
    private String bootstrapServers;

// consume message lÃ  string (key: string, value: string)
    @Bean
    public Map<String, String> consumerStringConfigs() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "group-id");
        // this is config to parse data from kafka is String
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        // end
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "100");
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "15000");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");

        return new DefaultKafkaConsumerFactory<>(props);
    }

    @Bean
    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, String>> StringkafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerStringConfigs());
        return factory;
    }

// consume message lÃ  json (key: string, value: json)
    @Bean
    public ConsumerFactory<String, User> consumerJsonFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        // this is config to parse data from kafka is json but key is string
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        // end
        props.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS, ErrorHandlingDeserializer.class);

        // set default type is object in json
        props.put(JsonDeserializer.VALUE_DEFAULT_TYPE, User.class);
        // end
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        
        return new DefaultKafkaConsumerFactory<>(props);
    }

    @Bean
    KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, User>> jsonKafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, User> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerJsonFactory());
        return factory;
    }

}

```
## Máº·c Ä‘á»‹nh sáº½ láº¯ng nghe vá» dáº¡ng string, nhÆ°ng Ä‘á»ƒ config láº¯ng nghe vá» dáº¡ng json

```java

config láº¯ng nghe vá» dáº¡ng json

@KafkaListener(topics = Constant.KAFKA, groupId = "${groupid.consumer.test:local_if_null}", containerFactory = "jsonKafkaListenerContainerFactory")


config láº¯ng nghe vá» dáº¡ng string

@KafkaListener(topics = Constant.KAFKA, groupId = "${groupid.consumer.test:local_if_null}", containerFactory = "stringKafkaListenerContainerFactory")
hoáº·c
@KafkaListener(topics = Constant.KAFKA, groupId = "${groupid.consumer.test:local_if_null}")

```


# Config producer

Náº¿u láº¯ng nghe kafka dáº¡ng <string, string> thÃ¬ dá»¯ liá»‡u Ä‘áº©y lÃªn cÅ©ng pháº£i Ä‘c Ä‘á»“ng bá»™ dang <string, string>

Náº¿u láº¯ng nghe kafka dáº¡ng <string, json> thÃ¬ dá»¯ liá»‡u Ä‘áº©y lÃªn cÅ©ng pháº£i Ä‘c Ä‘á»“ng bá»™ dang <string, json>

```java
@Configuration
class KafkaProducerConfig {

    @Value("localhost:9092")
    private String bootstrapServers;

// data push to kafka is string (key: string, value: string)
    @Bean
    public ProducerFactory<String, String> producerStringFactory() {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(
                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
        configProps.put(
                ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        configProps.put(
                ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        return new DefaultKafkaProducerFactory<>(configProps);
    }

    @Bean
    public KafkaTemplate<String, String> kafkaStringTemplate() { // inject kafkaStringTemplate to send to server
        return new KafkaTemplate<>(producerStringFactory());
    }

// data in Json (key: string, value: json)
@Bean
    public ProducerFactory<String, User> producerJsonFactory() {
    // public ProducerFactory<String, String> producerJsonFactory() { // also OK if we use `KafkaTemplate<String, String> kafkaJsonTemplate()`
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(
                ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapAddress);
        configProps.put(
                ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        configProps.put(
                ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        return new DefaultKafkaProducerFactory<>(configProps);
    }

    @Bean
    public KafkaTemplate<String, User> kafkaJsonTemplate() { // inject kafkaJsonTemplate to send to server
        return new KafkaTemplate<>(producerJsonFactory());
    }

```

config to read in special partition of topic
```java
application.yml
groupid.consumer: UATgroupId

/* Consumer */

    @KafkaListener(topics = Constant.KAFKA.TEST_TOPIC, groupId = "${groupid.consumer:local_group_id}")
    @KafkaListener(
            topicPartitions = {
                    @TopicPartition(
                            topic = "demo",
                            partitionOffsets = {
                                    @PartitionOffset(partition = "0", initialOffset = "1")
                            }),
                    @TopicPartition(
                            topic = "topic1",
                            partitionOffsets = {
                                    @PartitionOffset(partition = "1", initialOffset = "0")
                            })
            },
            groupId = "${groupid.consumer:default_groupId_name}
        ) // náº¿u cÃ³ define groupid.consumer: UATgroupId thÃ¬ sáº½ láº¯ng nghe. cÃ²n ko thÃ¬ sáº½ lÃ  default_groupId_name
        
    public void listen(String message) {
        System.out.println("Received Message in group - group-id data: " + message);
    }
    
    
/* producer */

 public void sendMessage(String topic, int partition, String message) {
//        ListenableFuture<SendResult<String, String >> future = kafkaTemplate.send(topic,  message);
        ListenableFuture<SendResult<String, String >> future = kafkaTemplate.send(topic, partition, "key",  message);
        try {
            log.info("check request with timeout 10s");
            future.get(10, TimeUnit.SECONDS);
            log.info("Send request is done: " + future.isDone());
        }
        catch (Exception e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

        // add callback
        future.addCallback(new ListenableFutureCallback<SendResult<String, String>>() {
            @Override
            public void onSuccess(SendResult<String, String> result) {
                log.info("call send successful!");
            }

            @SneakyThrows
            @Override
            public void onFailure(Throwable ex) {
                throw new Exception("fail");
            }
        });
    }
```

## @KafkaListener

- use `@RetryableTopic` to setup retry times

@RetryableTopic(attempts = "3", backoff = @Backoff(delay = 1000))

- use `concurrency` to set thread: You can specify the number of threads to run in the listener containers by setting the `spring.kafka.listener.concurrency`.
    
    @KafkaListener(topics = "myTopic", groupId = "myGroup", concurrency = "3")


# how many maximum number consumer of kafka?

If we add more consumers to a single group with a single topic than we have partitions, some of the consumers will be idle and get no messages at all

<img src="blog/java/img/kafka3.png" style="display: block; margin-right: auto; margin-left: auto;">

https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html



# ğ–ğ¡ğ² ğ¢ğ¬ ğŠğšğŸğ¤ğš ğ’ğ¨ ğ…ğšğ¬ğ­?

Kafka is known for its high performance and speed due to several reasons:

The two most important reasons are its low latency message delivery through Sequential I/O and Zero Copy Principle.

ğ—¦ğ—²ğ—¾ğ˜‚ğ—²ğ—»ğ˜ğ—¶ğ—®ğ—¹ ğ—œğ—¢:

Kafka relies heavily on the filesystem for storing and caching messages. The general perception is that "disks are slow, "meaning high seek time. Imagine if we can avoid seeking time. We can achieve low latency as low as RAM here. Kafka does this through Sequential I/O. Kafka efficiently uses log, an append-only, totally ordered data structure.

ğ—§ğ—¿ğ—®ğ—±ğ—¶ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ——ğ—®ğ˜ğ—® ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—²ğ—¿

To read a file from a disk, then send it over the network, a traditional data transfer would require four context switches between user and kernel modes, making the data copied four times.

The transfer process consists of four steps below:

1) The File. read() call makes the context switch from user mode to kernel mode, and the data copy is performed by the direct memory access (DMA) engine, which reads the file content from the disk and stores it into a kernel address space buffer (OS buffer);

2) Data is copied from the kernel space buffer into the user buffer(Application buffer), making the File.read() call return and the context to switch back to user mode

3) The Socket.send() call makes the context switch to kernel mode, and the third data copy is performed from the user buffer(application buffer) to the kernel buffer(socket Buffer) again

4) Finally, the DMA engine performs a fourth copy of the data, passing it from the kernel buffer (socket buffer) to the network interface controller (NIC) buffer to be sent over the network.

If the requested data is larger than the kernel buffer size, there will be even more copies between kernel and user spaces. Zero-copy optimisation reduces these redundant data copies.

ğ—­ğ—˜ğ—¥ğ—¢ ğ—–ğ—¢ğ—£ğ—¬

A zero-copy optimisation consists of removing the second and third data copies, making the data transferred directly from the OS buffer to the nic buffer.

Kafka uses this zero-copy principle by requesting the kernel to move the data to nic rather than via the application.

Zero Copy is a shortcut to save multiple data copies between the application and kernel contexts. This approach brings down the time by â€‹â€‹approximately 65%.

What else?

Kafka uses many other techniques apart from the ones mentioned above to make systems much faster and more efficient:

ğ— ğ—²ğ˜€ğ˜€ğ—®ğ—´ğ—² ğ—–ğ—¼ğ—ºğ—½ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—»: Kafka allows for message compression, which reduces messages' size and enables faster data transmission and processing.
Message Batching: Kafka can batch messages together, which reduces the overhead of individual message processing and improves throughput.

Overall, Kafka's speed results from its optimised architecture(sequential io, append-only log, Zero Copy), compression, batching, in-memory storage.


<img src="blog/java/img/kafka4.png" style="display: block; margin-right: auto; margin-left: auto;">

# Is Apache Kafka Providing Real Message Ordering?

Letâ€™s imagine the situation below: a single producer sending three messages to a topic:

- Message 1, for some reason, finds a long network route to Apache Kafka
- Message 2 finds the quickest network route to Apache Kafka
- Message 3 gets lost in the network

<img src="blog/java/img/kafka5.png" style="display: block; margin-right: auto; margin-left: auto;">

Even in this basic scenario, with only one producer, we could get an unexpected series of messages on the topic. The end result on the Kafka topic will show only two events being stored, with the unexpected ordering `2`, `1`.

## Acks and Retries

But not all is lost! If we look into the producing libraries (aiokafka being an example), we have ways to ensure that messages are delivered properly.

First of all, to avoid the problem with the message `3` in the above scenario, we could define a proper acknowledgment mechanism. The acks producer parameter allows us to define what confirmation of message reception we want to have from Apache Kafka.

Setting this parameter to `1` will ensure that we receive an acknowledgment from the primary broker responsible for the topic (and partition). Setting it to all will ensure that we receive the `ack` only if both the primary and the replicas correctly store the message, thus saving us from problems when only the primary receives the message and then fails before propagating it to the replicas.

Once we set a sensible `ack`, we should set the possibility to retry sending the message if we donâ€™t receive a proper acknowledgment. Differently from other libraries (kafka-python being one of them), aiokafka will retry sending the message automatically until the timeout (set by the `request_timeout_ms` parameter) has been exceeded.

With acknowledgment and automatic retries, we should solve the problem of the message `3`. The first time it is sent, the producer will not receive the ackTherefore, after the `retry_backoff_ms` interval, it will send the message `3` again.

<img src="blog/java/img/kafka6.png" style="display: block; margin-right: auto; margin-left: auto;">

## Max In-Flight Requests

However, if you watch the end result in the Apache Kafka topic closely, the resulting ordering is not correct: we sent `1`,`2`,`3` and got `2`,`1`,`3` in the topicâ€¦ how to fix that?

The old method (available in kafka-python) was to set the maximum in-flight request per connection: the number of messages we allow to be â€œin the airâ€ at the same time without acknowledgment. The more messages we allow in the air at the same time, the more risk of getting out-of-order messages.

When using kafka-python, if we absolutely needed to have a specific ordering in the topic, we were forced to limit the `max_in_flight_requests_per_connection` to `1`. Basically, supposing that we set the ack parameter to at least `1`, we were waiting for an acknowledgment of every single message (or batch of messages if the message size is less than the batch size) before sending the following one.

<img src="blog/java/img/kafka7.png" style="display: block; margin-right: auto; margin-left: auto;">

# Increase Complexity With Multiple Producers

## Different Locations, Different Latency

Again, the network is not equal, and with several producers located in possibly very remote positions, the different latency means that the Kafka ordering could differ from the one based on event time.

<img src="blog/java/img/kafka8.png" style="display: block; margin-right: auto; margin-left: auto;">

## Batching, an Additional Variable

To achieve higher throughput, we might want to batch messages. With batching, we send messages in â€œgroups,â€ minimizing the overall number of calls and increasing the payload to overall message size ratio. But, in doing so, we can again alter the ordering of events. The messages in Apache Kafka will be stored per batch, depending on the batch ingestion time. Therefore, the ordering of messages will be correct per batch, but different batches could have different ordered messages within them.

<img src="blog/java/img/kafka9.png" style="display: block; margin-right: auto; margin-left: auto;">

# The Savior: Event Time


We understood that the original premise about Kafka keeping the message ordering is not 100% true. The ordering of the messages depends on the Kafka ingestion time and not on the event generation time. But what if the ordering based on event time is important?

Well, we canâ€™t fix the problem on the production side, but we can do it on the consumer side. All the most common tools that work with Apache Kafka have the ability to define which field to use as event time, including [Kafka Streams](https://kafka.apache.org/0110/documentation/streams/core-concepts#streams_time), Kafka Connect with the dedicated Timestamp extractor single message transformation (SMT), and [Apache FlinkÂ®](https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/concepts/time/).

Consumers, when properly defined, will be able to reshuffle the ordering of messages coming from a particular Apache Kafka topic. Letâ€™s analyze the Apache Flink example below:

```java
CREATE TABLE CPU_IN (
    hostname STRING,
    cpu STRING,
    usage DOUBLE,
    occurred_at BIGINT,
    time_ltz AS TO_TIMESTAMP_LTZ(occurred_at, 3),
    WATERMARK FOR time_ltz AS time_ltz - INTERVAL '10' SECOND
    )
WITH (
   'connector' = 'kafka',
   'properties.bootstrap.servers' = '',
   'topic' = 'cpu_load_stats_real',
   'value.format' = 'json',
   'scan.startup.mode' = 'earliest-offset'
)
```

In the above Apache Flink table definition, we can notice:

- `occurred_at`: the field is defined in the source Apache Kafka topic in unix time (datatype is BIGINT).
- `time_ltz AS TO_TIMESTAMP_LTZ(occurred_at, 3)`: transforms the unix time into the Flink timestamp.
- `WATERMARK FOR time_ltz AS time_ltz - INTERVAL '10' SECOND` defines the new `time_ltz` field (calculated from `occurred_at`) as the event time and defines a threshold for late arrival of events with a maximum of 10 seconds delay.
Once the above table is defined, the `time_ltz` field can then be used to correctly order events and define aggregation windows, making sure that all events within the accepted latency are included in the calculations.

The `- INTERVAL '10' SECOND` defines the latency of the data pipeline and is the penalty we need to include to allow the correct ingestion of late-arriving events. Please note, however, that the throughput is not impacted. We can have as many messages flowing in our pipeline as we want, but weâ€™re â€œwaiting 10 secondsâ€ before calculating any final KPI in order to make sure we include in the picture all the events in a specific timeframe.

An alternative approach that works only if the events contain the full state is to keep a certain key (`hostname` and `cpu` in the above example) the maximum event time reached so far, and only accept changes where the new event time is greater than the maximum.


